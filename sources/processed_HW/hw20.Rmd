# Project 2

## Project Overview

In Project 2, you will select a data set, clean it and engineer features as needed, create and compare several distinct models of the relationship of a significant response variable to predictors, and report on your findings.  You may choose to work in a team of up to 3 members or you may work alone.  **Project 2 is due Wednesday December 14 by 12:30PM**, the end of the final exam period for our course.

You may use the same data set in Project 2 that you used in Project 1, or you may take the opportunity to explore a new data set.  The data set you select must allow you to carry out the activities and meet the criteria below.

### Select a data set

You will be making a predictive model.  Your data set must therefore have a good number of records (rows), several hundred at minimum.  It should also have a good number of features (columns).  Your data set must also have a response variable that is of clear significance to someone making decisions.  *Don't ask me about this aspect of your data set.  If you have to ask whether your data set has a clearly significant response variable, it probably doesn't*.  The data type of your response variable determines the type of model you will create: a numerical response requires a regression model and a categorical response requires a classification model.

### Clean the data and engineer features

Clean the data as needed.  Engineer new features from existing features.  One-hot encoding to signal the presence of words or phrases in text data, days of the week or months of the year from date data, and so on.  Of course, **before you do this, you should carry out a train/test split** to be in compliance with best practices.

### Create several models

Create several models of the response ~ predictors relationship.  You can consider models based on linear models with different relationships, generalized linear models or generalized additive models, decision trees, random forests, or even ensemble models.

### Assess and compare the models

Assess your models with a common loss function and compare their performance on the train and test sets.

### Report on your activities

For this project, you will need to produce two reports.  The first should be a fairly complete report on your activities.  The second should be an audiovisual report using slides, limited to a five minute presentation length.

The complete report should be written for a junior/senior level student in the natural/social sciences who has familiarity with statistical software and jargon.  The complete report should document data collection/assembly if applicable, method of train/test split and any archiving, data cleaning and feature engineering steps, creation and assessment of models, and conclusions. 

Both reports should clearly report your findings, stating conclusions as
such and raising questions for further study.  These reports should be very clear about the benefits and drawbacks of the models you explored.

## Project 2 Advice

Get started early. As part of Homework 10, I’ll ask you to identify your data set, so I’m doing my part to help you get moving!

You may need to engineer more features to create good models once you have trained your initial models, and you may need to adjust model hyperparameters.  This will be an iterative process, requiring creativity, patience, attention to detail, and time.  Make sure you leave yourself enough time.

## Project 2 Rubric

Project 2 is worth 10 points.

* 1 point - The selected data set exhibits the features detailed above.  In particular, there should be a response variable of clear and significant interest.
* 1 point - The complete report clearly identifies the response variable of interest, clearly notes its type (numerical or categorical) and clearly notes the type of models that will be created (regression or classification).
* 1 point - The complete report exhibits good grammar and clarity, and is written for the appropriate audience.
* 1 point - The complete report clearly states the origins of the data set, and documents data cleaning efforts and feature engineering.
* 1 point - The complete report shows evidence of good practices in predictive modeling, and especially documents a train/test split and a cleaning/feature engineering pipeline.
* 1 point - The complete report documents the creation of at least three predictive models from distinct theoretical sources.
* 1 point - The complete report documents the assessment and comparison of the predictive models created.
* 1 point - The AV report exhibits clarity and brevity.
* 1 point - The AV report uses good graphics and tables to support analysis and conclusions.
* 1 point - Both reports clearly state findings, both conclusions and questions for further study.