# The Tenth Homework: The Train/Test Framework and First Steps Away from Linear Models


## Train/Test Split Errors - Non-Random Split

In this question we want to explore why it is important to use random sampling to split data into train/test sets.  Data often appears in data sets in sorted or grouped rows.  For example, the data in the `iris` data set is grouped by the `Species` variable.  Let's look at some synthetic data, sort it on one variable, make the train/test split incorrectly, and see what happens!

(a) Execute the following code to create a synthetic data set `df` sorted on the `x` variable.
```{r}
set.seed(1234)
x <- runif(100)
noise <- rnorm(length(x),mean=0,sd=0.02)
y <- 1-(x-0.75)^2 + noise
df <- data.frame(x=x,y=y)
df <- df[order(df$x),]
```

(b) Plot the data, `y` versus `x`.

(c) We want to make the train/test split, but in this part we will make it **incorrectly**.  Create a training data set, `train_wrong`, and a test data set `test_wrong` by taking the first fifty and the last fifty rows in `df`, respectively.
```{r,echo=FALSE}
train_wrong <- df[1:50,]
test_wrong <- df[51:nrow(df),]
```

(d) Now plot the data in `train_wrong`.  Answer:
```{r,echo=FALSE}
library(ggplot2)
ggplot(data=train_wrong,aes(x=x,y=y)) + geom_point()
```

(e) Great, looks like a linear relationship!  Let's fit a linear model `lin.mod` to the data in `train_wrong` using the formula `y~x`.  What is the $R^2$ value?  Answer:
```{r,echo=FALSE}
lin.mod <- lm(y~x,data=train_wrong)
summary(lin.mod)$r.squared
```

(f) Note that we can compute the $R^2$ value using a formula.  Try executing the following code.
```{r}
y.predicted <- predict(lin.mod)
y.actual <- train_wrong$y
mu.predicted <- mean(y.predicted)
mu.actual <- mean(y.actual)
numerator <- (sum((y.predicted - mu.predicted)*(y.actual - mu.actual)))^2
denominator <- (sum((y.predicted-mu.predicted)^2)*sum((y.actual-mu.actual)^2))
numerator/denominator
```

In R you can also use
```{r}
cor(y.predicted,y.actual)^2
```

(g) Okay, it's time to evaluate our model on our test set `test_wrong`.  Execute the following code, making sure you understand what is going on!
```{r}
y.predicted <- predict(lin.mod,test_wrong)
y.actual <- test_wrong$y
cor(y.predicted,y.actual)^2
```

(h) Oof.  Why is $R^2$ so much lower?  Let's plot the whole data set together with predictions across all the data.  Execute the following code:
```{r}
df$y.predicted <- predict(lin.mod,df)
df$traintest <- c(rep("train",50),rep("test",50))
ggplot(data=df,aes(x=x,y=y,col=traintest)) +
  geom_point() +
  geom_line(aes(x=x,y=y.predicted),col="black")
```

(i) Okay, what's going on here?  Write "the moral of the story" for this problem.

*Note: This is not an unusual scenario.  Data is often sorted on one variable or another.  Random sampling for train/test sets helps to avoid bias.*


## Train/Test Split - Correcting the Previous Exercise

Using the data set from Exercise 1...

(a) Execute the following code to *correctly* perform a 50/50 train/test split.
```{r}
n <- round(nrow(df) * 0.5)
in_train <- sample(1:nrow(df),n)
train <- df[in_train,]
test <- df[-in_train,]
```

(b) Plot `y` versus `x` for the `train` data set.  What linear model is indicated?

(c) Fit a linear model (your choice on relationship) to the `train` data.  Compute $R^2$.  Answer for my choice of model:
```{r,echo=FALSE}
lin.mod <- lm(y~x+I(x^2), data=train)
summary(lin.mod)$r.squared
```

(d) Now evaluate your model's performance on your test data.  If the $R^2$ value is about the same, great!  If it's higher, you're probably just lucky; don't let it go to your head.  If much lower, then the model overfit the training data.  Here's my code for my `lin.mod` model:
```{r}
y.predicted <- predict(lin.mod,test)
y.actual <- test$y
cor(y.predicted,y.actual)^2
```

## Cubic Spline Model

(a) Download the IMDB data using the following code.
```{r,message=FALSE,warning=FALSE}
library(tidyverse)
df <- read.csv("https://github.com/cbrown-clu/Math-331-Data-Analysis/raw/master/B.%20Data%20sets/tmdb_5000_movies.csv") %>%
  select(original_title,budget,revenue)
```

(b) Split the data into train/test data using a 50/50 split.
```{r,echo=FALSE}
n <- round(nrow(df)*0.5)
in_train <- sample(1:nrow(df),n)
train <- df[in_train,]
test <- df[-in_train,]
```

(c) Plot `revenue` versus `budget` for the training data set.  My plot:
```{r,echo=FALSE}
library(ggplot2)
ggplot(data=train,aes(x=budget,y=revenue)) + geom_point()
```

(d) Let's fit a cubic spline model to this data with `df=4` as an option.  Try it!
```{r,echo=FALSE}
library(splines)
spline.mod <- lm(revenue ~ bs(budget,df=4),data=train)
```

(e) Compute $R^2$ for the training data.  My answer:
```{r,echo=FALSE}
y.predicted <- predict(spline.mod)
y.actual <- train$revenue
cor(y.predicted,y.actual)^2
```

(f) Not bad!  But how does the model perform on the test data?  Check $R^2$ for the test data.  My answer:
```{r,echo=FALSE}
y.predicted <- predict(spline.mod,test)
y.actual <- test$revenue
cor(y.predicted,y.actual)^2
```
Great!  It doesn't appear that we overfit our model.  *Note: You may see a warning about `x` values beyond boundary knots when making predictions on the test set.  This is because the ranges of the `x` values in `train` and `test` may be different.*


## Generalized Additive Model

Repeat exercise 3, but use a GAM with `family=gaussian()`.  You will need to load the `mgcv` library.
```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(mgcv)
gam.model <- gam(revenue ~ budget, family=gaussian(),data=train)
print("Training r squared:")
y.predicted <- predict(gam.model)
y.actual <- train$revenue
print(cor(y.predicted,y.actual)^2)
print("Test r squared:")
y.predicted <- predict(gam.model,test)
y.actual <- test$revenue
print(cor(y.predicted,y.actual)^2)
```
How does this compare to the $R^2$ value you achieved with cubic splines?

## Imbalanced Classes, GLM

(a) Execute the following code to load a data set:
```{r}
library(tidyverse)
df <- read.csv("https://github.com/cbrown-clu/Math-331-Data-Analysis/raw/master/B.%20Data%20sets/bank-additional-alternate.csv") %>%
  select(age, marital,default,housing,loan,subscribe) %>%
  mutate(marital=factor(marital),default=factor(default),
         housing=factor(housing),loan=factor(loan),
         subscribe=factor(subscribe))
```

(b) We would like to predict whether customers in this data set will `subscribe`.  Since this is a categorical variable, we will try to use a logistic regression model (glm with logit link function).  Before we fit this model, let's quickly examine the response variable. 
```{r}
summary(df$subscribe)
```

(c) We have an imbalanced class problem here.  What's the issue?  Well, if I simply predict that everyone is a "no" on subscribe, then according to the data I will only make an error in a fraction `r 4640/41188` of predictions, or a little over 11% of my answers will be errors.  So I have to beat this baseline.  Here is one approach: we'll simply sample in such a way that balances the yes/no classes in the training data set.  The following approach will result in a smaller training data set than we might like...but let's try it!
```{r,message=FALSE,warning=FALSE}
library(tidyverse)
df_yes <- df[df$subscribe=="yes",]
n_yes <- nrow(df_yes)
df_no <- df[df$subscribe=="no",]
n_train <- round(0.5 * n_yes)
in_train1 <- sample(1:nrow(df_yes),n_train)
train1 <- df_yes[in_train1,]
test1 <- df_yes[-in_train1,]
in_train2 <- sample(1:nrow(df_no),n_train)
train2 <- df_no[in_train2,]
test2 <- df_no[-in_train2,]
train <- bind_rows(train1,train2)
test <- bind_rows(test1,test2)
```

(d) Check that the `train` set has equal numbers of `yes` and `no` responses to `subscribe`.

(e) Create a logistic regression model for `subscribe` depending on `age` only (`glm` with a `family=binomial()`).  

```{r,echo=FALSE}
glm.mod <- glm(subscribe ~ age, family=binomial(),data=train)
```

(f) Create the table and assess the error rate for this model for the training set.  Remember, you'll need to set a cut point for where to decide whether predictions are converted to 1 or to 0.  My answer:
```{r,echo=FALSE}
y.predicted <- predict(glm.mod,type="response") > 0.518
y.actual <- train$subscribe == "yes"
temp <- table(y.predicted,y.actual)
percent <- (temp[1,2]+temp[2,1])/length(y.actual)
percent
```

So the error rate is about `r 100*percent`%.  This is *much* worse than the error rate I would have if I simply predict that everyone is a "no" on subscribe.  We need a better model!

