---
title: "Forty Days and Forty Nights of Data"
subtitle: "Exercises in R for Data Analysts"
author: "Dr. C. E. Brown"
date: "2023-07-07"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::pdf_book: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval=TRUE,
                      message=FALSE,
                      warning=FALSE,
                      comment = '    ',
                      class.source="bg.success",
                      class.output="bg.warning")
# The class.source and class.output
# are options for shading with html output
```

\newpage

## Preface, or, What is This Thing?

This thing is a collection of references and exercises for the aspiring data analyst, or scientist, or whatever we are calling them nowadays.  I hesitate to call it a book, as that hints at aspirations far beyond what you read.

Who are you?

* You're a student.  No, not someone enrolled at an institution of higher education, though you might be.  I mean that you're interested in studying.
* You might be in business, but you might not.
* You're on a budget.  You have limited financial resources and your computing resources are limited to your laptop.  All of the exercises in this thing can be run on a common laptop with an internet connection and using freely available tools.
* You like to figure things out and make connections.  When you run into a "Huh, that's weird!" situation, you look at it as an opportunity.
* You're aware that there is data all over the place, and you want to see what it can tell you.

Where did this thing come from?  I've been teaching an undergraduate course in data analysis for over a decade.  Most of these exercises were developed in one of two ways:

* I worked with a messy data set, completed my analysis, and then broke down the steps into sequences of exercises.  I scattered these throughout the homework sets so that each set builds on the earlier ones.
* I worked with a student with a messy understanding.  I created some data that isolated and clarified some ideas for that student, then found that those clarifications resonated with others.  This is where most of the synthetic data exercises came from.

How is this thing organized?  The first dozen chapters are the twelve homework sets for the class I teach, expanded somewhat, and in the order I present them.  Students have roughly a week to work through each homework set, and there are a few additional weeks in the semester given over to work on course projects.  The next few chapters are expansions on what I feel are both important tools for the data scientist and trouble spots for the learning practitioner.  The next few chapters after that are some longer case studies, taken step by step.  And the last two chapters include references to sources of data and to reference works.

Who am I?  I'm a mathematician by academic training and I have an inordinate fondness for differential equations and stochastic processes, but don't let that scare you.  I'm a data scientist in practice.  I'm still at heart a punk kid and D&D player from the 80's and 90's, with all of the DIY attitude that entails.  I've been coding since 1982, but I'm not nostalgic for the old days and I'm happy with modern fast computers and slick, easier-to-use languages.

## How Do I Use This Thing in my Class?

I assign one homework per week.  Thirteen homeworks plus half a dozen class periods for time to work on the two course projects fills up the fifteen week semester.  



Other notes:

* Depending on the class and their familiarity or lack thereof with coding, I might expand the time spent on the first homework set to two weeks and skip the thirteenth homework.
* Homework 13 is entirely optional.  While the subject material is fascinating and raises interesting questions both mathematical and philosophical, unsupervised learning is quite different from the earlier course material.
* Hypothesis testing does not comprise a large component of the course; this was not intended to be an introductory statistics course.  I don't recommend omitting the material in homework 8 entirely, as many metrics for model performance are couched in the language of p-values.  However, the exercises on power estimation and ANOVA can be omitted if time is short.
* I record video captures of myself working through various R/RStudio tasks.  This is difficult to capture in a static book form and so easy as a video!    

## Readings in the Course

Fischetti, Tony. *Data analysis with R: second edition*. Packt Publishing Ltd, 2018.  I find this an approachable read for my students coming from a non-coding background.

James, Gareth, et al. *An introduction to statistical learning with applications in R: second edition*. Vol. 112. New York: springer, 2021.  An absolute classic.  At the time I wrote these notes, this book was available in pdf form [at the authors' website](https://www.statlearning.com/).

Wickham, Hadley, and Garrett Grolemund. *R for data science: import, tidy, transform, visualize, and model data.*  O'Reilly Media, Inc., 2016.  This is a treasure trove of resources for practitioners who use the tidyverse.  At the time I wrote these notes, this book was [available freely on the web](https://r4ds.had.co.nz/) .

## Data Sets and Where to Find Them

Here are some sources for data sets for the course. The data available varies a good bit. You and your students should plan to spend some time exploring different data sets; you want to make sure that you select data that you and they are able to work with. In particular, the data should be

* Tabular.  CSV format is best.  Excel notebooks can be imported using the `readxl` library.  Some data sets are structured but not tabular and are usually stored in the .json file format.  Other data sets are unstructured; these might be collections of images, audio recordings, or video files. We do not address unstructured data in these notes.
* Relatively clean and well-formatted already.
* Of a size small enough for you to work with on your machine.  Memory is the real measure here; Gb of data may be difficult to work with on most personal machines.

Data sources:

* Kaggle has some great data sets to explore. Most are cleaned up already.  You will need a login.  At the time I wrote these notes, this was free.
* UCI Machine Learning Repository has some well-archived data sets.  Many of the data sets are stored as tabular data together with a metadata file.  The variable names are located in the metadata file and will need to be attached to the imported tabular data frame somehow.
* data.gov and data.ca.gov and dataLA provide open data access to non-sensitive government data sets.  Our institution is located in California and thus the listing of the CA-centric data sets.  Many other states/counties/cities have their own data hubs.  Explore!
* FRED is the home for the Federal Reserve Economic Data.
* NCAR/UCAR is the data archive for the National Center for Atmospheric Research.
* The FiveThirtyEight data page has a lot of interesting data sets tied to their articles on politics, sports, and economics.
* Google has a data set search tool. Be a bit cautious using this. It is free to use but not all of the data linked through it is free...and some data can be tremendously expensive.
* Datahub.io is another archive of open data sets that are mostly cleaned.
* The Earthdata site gives access to NASA's observations of Earth.
* CERN's Open Data site is not for the faint of heart.
* UN/WHO Global Health Observatory data repository.
* FBI Crime Data explorer.

## Additional Resources

* [Posit cheat sheets](https://posit.co/resources/cheatsheets/) for R/RStudio.  Solid gold.
* [Bookdown's home page](https://bookdown.org/) .  Bookdown is an extension of RMarkdown for creating books (these notes were compiled in bookdown).  Bookdown's page contains links to a good set of resource texts that were authored in bookdown.

```{r child="sources/day4.Rmd"}

```


```{r,echo=FALSE}
the_hw_names <- sapply(1:4,function(z) paste0("sources/day",z,".Rmd"))
```


<!-- ```{r, child=the_hw_names,echo=FALSE} -->

<!-- ``` -->

